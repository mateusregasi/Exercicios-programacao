{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7736243-239c-41e3-b325-68bf2bfd2aa5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tarefa A\n",
    "Na tarefa um, o objetivo vai ser: Dado um título e uma mensagem, queremos classificar qual a inclinação política de determinada pessoa.\n",
    "\n",
    "Vamos separar a tarefa em alguns passos:\n",
    "\n",
    "1. Criando ambiente virtual e importando bibliotecas\n",
    "2. Importar os dados\n",
    "3. Tokenizar os dados\n",
    "4. Usar o encoder para obter a representação vetorial das features\n",
    "5. Treinar o modelo de rede neural para classificação\n",
    "6. Métricas de avaliação\n",
    "7. Testes empíricos\n",
    "\n",
    "# Criando ambiente virtual e importando bibliotecas\n",
    "\n",
    "Para isso, rode os seguintes comandos:\n",
    "\n",
    "> python3 -m venv .venv\n",
    "> pip install -r requiriments.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28a717-2a2a-4157-a1b1-d33cfec23d19",
   "metadata": {},
   "source": [
    "# 2) Importar os dados\n",
    "\n",
    "Os dados estão em arquivos em formato de json. Vamos criar uma função que pega esses dados e importa como um map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78fea5d2-26e2-4de5-97c9-b33e77b3af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_json(path):\n",
    "    from json import loads\n",
    "    from os import listdir\n",
    "\n",
    "    names = list(\n",
    "        filter(\n",
    "            lambda x: x if not x.startswith(\".\") else None,\n",
    "            listdir(path)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "    for name in names:\n",
    "        file_name = f\"{path}/{name}\"\n",
    "        text = \"\".join(open(file_name).readlines())\n",
    "        json = loads(text)\n",
    "        data.append({\n",
    "            \"title\":json[\"title\"],\n",
    "            \"content\":json[\"content\"],\n",
    "            \"label\":json[\"label\"],\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620a27-4c91-46d4-9bbe-96a00139a156",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# 3) Tokeniza os dados\n",
    "\n",
    "Queremos tokenizar os dados que acabamos de importar, por isso, vamos criar uma função que vai fazer essa tokenização por nós."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c121ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mateusregasi/Documents/Exercicios-programacao/inteligencia_artificial/nlp/trabalho/TarefaA/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LENGTH_TOKENS = 256\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tokenize(string):\n",
    "    return tokenizer(\n",
    "        string,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH_TOKENS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598e4ad-c1e4-4359-b321-fed6b30ca6a9",
   "metadata": {},
   "source": [
    "# 4) Usar o encoder para obter a representação vetorial das features\n",
    "\n",
    "Antes de tudo, vamos escolher o dispositivo que usaremos para fazer os cálculos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c2ec70-acb3-4308-8dd2-5b788cb251bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f6f64-b065-4fa2-b39b-1dce0276f43b",
   "metadata": {},
   "source": [
    "Antes de iniciar o processo de formatação dos dados, vamos criar uma função que pega a representação dos tokens e encodifica na representação do nosso modelo. \n",
    "\n",
    "Vamos carregar o nosso modelo e associar a um dispositivo. Além disso, vamos colocar o modelo em modo de inferência, não de treinamento (para o Autocad não guardar o grafo computacional do gradiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda2a308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Model(\n",
       "  (embeddings): DebertaV2Embeddings(\n",
       "    (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (encoder): DebertaV2Encoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x DebertaV2Layer(\n",
       "        (attention): DebertaV2Attention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (pos_dropout): StableDropout()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "          (output): DebertaV2SelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaV2Intermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaV2Output(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model_encode = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model_encode.to(device)\n",
    "\n",
    "for param in model_encode.parameters():\n",
    "    param.requires_grad = False\n",
    "model_encode.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80777486",
   "metadata": {},
   "source": [
    "Vamos agora criar o código para encodificar os tokens com a saída contextual da última camada do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2e82cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens):\n",
    "    tokens = {k: v.to(model_encode.device) for k, v in tokens.items()}\n",
    "    return model_encode(**tokens).last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec6768b",
   "metadata": {},
   "source": [
    "Note que o DeBERTa tem uma representação vetorial relativa a quantidade de tokens. Precisamos juntar tudo isso em um único vetor. Vamos fazer isso utilizando uma técnica de pooling que utiliza média. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0505e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(last_hidden_state, attention_mask):\n",
    "    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float().to(device)\n",
    "    summed = (last_hidden_state * mask).sum(1)\n",
    "    counts = mask.sum(1).clamp(min=1e-9)\n",
    "    return (summed / counts).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe33e0",
   "metadata": {},
   "source": [
    "Vamos criar agora o código que faz a leitura de todos os dados dos arquivos json, aplica a tokenização e pega a representação vetorial contextual de cada par de título e mensagem. Vamos colocar tudo em um único map com também a label. Essa etapa inteira é para a normalização dos dados para o treinamento. As separações de dataset de treinamento, desenvolvimento e teste serão feitos no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78673fd1-f9d1-4b6e-ae46-afdba13ae6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZED_SUFIX_FILE = \"obj\"\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db71a636-9f6e-4939-9816-44d0c6868c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"dev_json\",\n",
    "    \"train_json\"\n",
    "]\n",
    "c = 0\n",
    "for path in paths:\n",
    "    data = import_json(path)\n",
    "    with torch.no_grad():\n",
    "        for d in data:\n",
    "            tokenized_title = tokenize(d[\"title\"])\n",
    "            tokenized_content = tokenize(d[\"content\"])\n",
    "            \n",
    "            title = mean_pooling(\n",
    "                encode(tokenized_title),\n",
    "                tokenized_title[\"attention_mask\"]\n",
    "            ).squeeze()\n",
    "            \n",
    "            content = mean_pooling(\n",
    "                encode(tokenized_content),\n",
    "                tokenized_content[\"attention_mask\"]\n",
    "            ).squeeze()\n",
    "\n",
    "            example = {\n",
    "                \"title\":title,\n",
    "                \"content\":content,\n",
    "                \"label\":torch.tensor(d[\"label\"], dtype=torch.long)\n",
    "            }\n",
    "\n",
    "            torch.save(\n",
    "                example,\n",
    "                DATA_PATH + NORMALIZED_SUFIX_FILE + str(c)\n",
    "            )\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391a146-bf0c-4c32-95b3-4e5092b24f39",
   "metadata": {},
   "source": [
    "# 5) Treinar o modelo de rede neural para classificação\n",
    "\n",
    "Vamos inicialmente preparar o dataset para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, device):\n",
    "        self.device = device\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.files[idx]\n",
    "        example = torch.load(\n",
    "            self.files[idx], \n",
    "            map_location=self.device\n",
    "        )\n",
    "        return example[\"title\"], example[\"content\"], example[\"label\"]\n",
    "\n",
    "\n",
    "files = [os.path.join(DATA_PATH, f) for f in os.listdir(DATA_PATH)]\n",
    "\n",
    "train_files = files[:len(files)//11 * 9]\n",
    "dev_files = files[len(files)//11 * 9:len(files)//11 * 10]\n",
    "test_files = files[len(files)//11 * 10:]\n",
    "del files\n",
    "\n",
    "train_set = MyDataset(train_files, device)\n",
    "test_set = MyDataset(test_files, device)\n",
    "dev_set = MyDataset(dev_files, device)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dev_loader = torch.utils.data.DataLoader(\n",
    "    dev_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "with open(\"json_model_data.json\", \"r\") as f:\n",
    "    from json import load\n",
    "    modelB_file = load(f)\n",
    "modelB_set = MyDataset(dev_files, device)\n",
    "modelB_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c260b2-d48c-44ef-9212-7271279b3fad",
   "metadata": {},
   "source": [
    "Em seguida, vamos criar o modelo. Ele vai receber o vetor de contexto do título e da mensagem e devolver os logits da última camada (para utilizar, posteriormente, o softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c51ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self, hidden=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        dropout= 0.2\n",
    "        input_dim = hidden * 2\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.LayerNorm(hidden),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(hidden, hidden // 2),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(hidden // 2, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, title_emb, message_emb):\n",
    "        x = torch.cat([title_emb, message_emb], dim=-1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "classifier = ClassifierModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d628e-f272-4e59-b737-8b307602a389",
   "metadata": {},
   "source": [
    "Já temos uma classe que gerencia o dataset e pega os dados já transformados. Já temos o modelo de inferência. Resta agora preparar alguns aspectos de arquitetura para o treinamento. Precisamos definir:\n",
    "\n",
    "1. Função de Custo\n",
    "2. Otimizador\n",
    "3. Escalonador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "27666cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-3, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbea7a-c469-419f-83a7-5f1480c91405",
   "metadata": {},
   "source": [
    "Agora, resta treinar o modelo. Vamos preparar um treinamento em batch com earling stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67baeb3d-fceb-4257-aa3d-9e3f48d5d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "aa7ccb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 — train_loss 0.9241 — accuracy 0.5702 — precision 0.5533 — recall 0.5533 — val_loss 0.7912 — val_acc 0.6421\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.7912)\n",
      "epoch 1 — train_loss 0.8054 — accuracy 0.6421 — precision 0.6296 — recall 0.6296 — val_loss 0.8727 — val_acc 0.6085\n",
      "  → Sem melhora (1/5)\n",
      "epoch 2 — train_loss 0.7712 — accuracy 0.6585 — precision 0.6464 — recall 0.6464 — val_loss 0.7159 — val_acc 0.6874\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.7159)\n",
      "epoch 3 — train_loss 0.7347 — accuracy 0.6785 — precision 0.6689 — recall 0.6689 — val_loss 0.7244 — val_acc 0.6982\n",
      "  → Sem melhora (1/5)\n",
      "epoch 4 — train_loss 0.7081 — accuracy 0.6946 — precision 0.6852 — recall 0.6852 — val_loss 0.9275 — val_acc 0.6257\n",
      "  → Sem melhora (2/5)\n",
      "epoch 5 — train_loss 0.7018 — accuracy 0.7056 — precision 0.6964 — recall 0.6964 — val_loss 0.7117 — val_acc 0.6913\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.7117)\n",
      "epoch 6 — train_loss 0.6847 — accuracy 0.7091 — precision 0.6994 — recall 0.6994 — val_loss 0.6745 — val_acc 0.7056\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.6745)\n",
      "epoch 7 — train_loss 0.6523 — accuracy 0.7234 — precision 0.7152 — recall 0.7152 — val_loss 0.6529 — val_acc 0.7256\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.6529)\n",
      "epoch 8 — train_loss 0.6299 — accuracy 0.7351 — precision 0.7279 — recall 0.7279 — val_loss 0.6441 — val_acc 0.7296\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.6441)\n",
      "epoch 9 — train_loss 0.6233 — accuracy 0.7363 — precision 0.7287 — recall 0.7287 — val_loss 0.6436 — val_acc 0.7291\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.6436)\n",
      "epoch 10 — train_loss 0.6198 — accuracy 0.7395 — precision 0.7326 — recall 0.7326 — val_loss 0.6436 — val_acc 0.7291\n",
      "  → Sem melhora (1/5)\n",
      "epoch 11 — train_loss 0.6188 — accuracy 0.7399 — precision 0.7327 — recall 0.7327 — val_loss 0.6421 — val_acc 0.7276\n",
      "  ✓ Melhor modelo salvo (val_loss: 0.6421)\n",
      "epoch 12 — train_loss 0.6210 — accuracy 0.7388 — precision 0.7310 — recall 0.7310 — val_loss 0.6449 — val_acc 0.7276\n",
      "  → Sem melhora (1/5)\n",
      "epoch 13 — train_loss 0.6275 — accuracy 0.7357 — precision 0.7291 — recall 0.7291 — val_loss 0.6531 — val_acc 0.7274\n",
      "  → Sem melhora (2/5)\n",
      "epoch 14 — train_loss 0.6336 — accuracy 0.7331 — precision 0.7260 — recall 0.7260 — val_loss 0.6523 — val_acc 0.7223\n",
      "  → Sem melhora (3/5)\n",
      "epoch 15 — train_loss 0.6387 — accuracy 0.7298 — precision 0.7224 — recall 0.7224 — val_loss 0.6708 — val_acc 0.7195\n",
      "  → Sem melhora (4/5)\n",
      "epoch 16 — train_loss 0.6437 — accuracy 0.7263 — precision 0.7187 — recall 0.7187 — val_loss 0.8419 — val_acc 0.6360\n",
      "  → Sem melhora (5/5)\n",
      "\n",
      "⛔ Early stopping acionado após 17 epochs!\n",
      "\n",
      "Modelo final carregado: classifier_model.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_preds = []\n",
    "    epoch_labels = []\n",
    "    \n",
    "    for title, content, label in train_loader:\n",
    "        title, content, label = title.to(device), content.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = classifier(title, content)\n",
    "        loss = criterion(logits, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        labels_np = label.cpu().numpy()\n",
    "        epoch_preds.extend(preds)\n",
    "        epoch_labels.extend(labels_np)\n",
    "    \n",
    "    accuracy = np.mean(np.array(epoch_preds) == np.array(epoch_labels))\n",
    "    precision = np.mean([np.mean(np.array(epoch_preds)[np.array(epoch_labels) == i] == i) for i in range(3) if np.sum(np.array(epoch_labels) == i) > 0])\n",
    "    recall = np.mean([np.mean(np.array(epoch_preds)[np.array(epoch_labels) == i] == i) for i in range(3) if np.sum(np.array(epoch_labels) == i) > 0])\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    classifier.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for title, content, label in dev_loader:\n",
    "            title, content, label = title.to(device), content.to(device), label.to(device)\n",
    "            logits = classifier(title, content)\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels_np = label.cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(labels_np)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(dev_loader)\n",
    "    val_accuracy = np.mean(np.array(val_preds) == np.array(val_labels))\n",
    "    \n",
    "    print(f\"epoch {epoch} — train_loss {avg_loss:.4f} — accuracy {accuracy:.4f} — precision {precision:.4f} — recall {recall:.4f} — val_loss {avg_val_loss:.4f} — val_acc {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(classifier.state_dict(), \"classifier_model.pt\")\n",
    "        print(f\"  ✓ Melhor modelo salvo (val_loss: {avg_val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  → Sem melhora ({patience_counter}/{patience})\")\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n⛔ Early stopping acionado após {epoch + 1} epochs!\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\\nModelo final carregado: classifier_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9761864-203a-4ae1-a7fe-6d9e0a80af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Métricas de avaliação\n",
    "\n",
    "Vamos agora utilizar métricas de avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb58fa90-066e-43f0-a8c9-ced4cc96b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(\"classifier_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dac089-fd86-409a-99b1-31fa16567343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_epochs = len(test_set) // BATCH_SIZE\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    classifier.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for title, content, label in test_loader:\n",
    "            title, content, label = title.to(device), content.to(device), label.to(device)\n",
    "            logits = classifier(title, content)\n",
    "            \n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            labels_np = label.cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels_np)\n",
    "\n",
    "test_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "test_precision = np.mean([np.mean(np.array(all_preds)[np.array(all_labels) == i] == i) for i in range(3) if np.sum(np.array(all_labels) == i) > 0])\n",
    "test_recall = np.mean([np.mean(np.array(all_preds)[np.array(all_labels) == i] == i) for i in range(3) if np.sum(np.array(all_labels) == i) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5aff3b3-b6e0-4f4e-b781-d1dc040fe1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7276 — precision 0.7198 — recall 0.7198\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy {test_accuracy:.4f} — precision {test_precision:.4f} — recall {test_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386623d2-6602-49af-a87e-8bc014f3bbf8",
   "metadata": {},
   "source": [
    "Vamos tentar verificar agora se tem alguma diferença de performance relacionada ao alinhamento político. Para fazer isso vamos utilizar macroaveraging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdfb3e-8009-47e4-835d-08c60c39158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> \n",
      "precision (0.6528925619834711)\n",
      "recall (0.7022222222222222)\n",
      "f-measure (0.676659528907923)\n",
      "\n",
      "1 -> \n",
      "precision (0.731457800511509)\n",
      "recall (0.7606382978723404)\n",
      "f-measure (0.7457627118644068)\n",
      "\n",
      "2 -> \n",
      "precision (0.7750281214848144)\n",
      "recall (0.7165886635465418)\n",
      "f-measure (0.7446636044312347)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y é o valor predito e x é o valor real\n",
    "confuse_matrix = np.zeros([3,3])\n",
    "for i in range(len(all_labels)):\n",
    "    confuse_matrix[all_preds[i]][all_labels[i]] += 1 \n",
    "\n",
    "def precision(matrix, i): return matrix[i][i] / (matrix[0][i] + matrix[1][i] + matrix[2][i])\n",
    "def recall(matrix, i): return matrix[i][i] / (matrix[i][0] + matrix[i][1] + matrix[i][2])\n",
    "def fmeasure(p, r, b): return (b**2 + 1) * p*r / (b**2 * p + r)\n",
    "\n",
    "for i in range(3):\n",
    "    p = precision(confuse_matrix, i)\n",
    "    r = recall(confuse_matrix, i)\n",
    "    b = 1\n",
    "    print(f\"{i} -> \\nprecision ({p})\")\n",
    "    print(f'recall ({r})')\n",
    "    print(f'f-measure ({fmeasure(p, r, b)})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a36ad6-6d16-4e51-bc88-16e91cdb19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Testes empíricos\n",
    "\n",
    "Agora podemos brincar com o modelo! Vamos fazer, inicialmente, uma função que dá a resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6ff0c952-92b2-4d50-8330-2d8e88d767cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = [\n",
    "    \"esquerda\",\n",
    "    \"centro\",\n",
    "    \"direita\"\n",
    "]\n",
    "\n",
    "def predict(content, title, return_probs=False, label_map=None):\n",
    "    tokenized_title = tokenize(title)\n",
    "    tokenized_content = tokenize(content)\n",
    "\n",
    "    title_emb = mean_pooling(\n",
    "        encode(tokenized_title),\n",
    "        tokenized_title[\"attention_mask\"]\n",
    "    )\n",
    "    content_emb = mean_pooling(\n",
    "        encode(tokenized_content),\n",
    "        tokenized_content[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(title_emb, content_emb)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        pred_idx = int(probs.argmax(axis=1)[0])\n",
    "\n",
    "    return label_map[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9be9cad1-2d31-490f-90ee-9eb4f3847157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esquerda\n"
     ]
    }
   ],
   "source": [
    "content = \"News\\n\\nThe Number Of Desperate Immigrants Who Die While Trying To Get Into The US Keeps Rising\"\n",
    "title = \"The questions that 800,000 people are waiting for Trump and Jeff Sessions to answer about DACA\"\n",
    "print(predict(content, title, label_map=label_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29551732",
   "metadata": {},
   "source": [
    "# Parte dedicada a testar o modelo B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"json_model_data.json\", \"r\") as f:\n",
    "    from json import load\n",
    "    modelB_file = load(f)\n",
    "modelB_set = MyDataset(dev_files, device)\n",
    "modelB_loader = torch.utils.data.DataLoader(\n",
    "    modelB_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval(loader):\n",
    "    num_epochs = len(loader) // BATCH_SIZE\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for title, content, label in loader:\n",
    "                title, content, label = title.to(device), content.to(device), label.to(device)\n",
    "                logits = classifier(title, content)\n",
    "                \n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                labels_np = label.cpu().numpy()\n",
    "                \n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels_np)\n",
    "\n",
    "    test_accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "    test_precision = np.mean([np.mean(np.array(all_preds)[np.array(all_labels) == i] == i) for i in range(3) if np.sum(np.array(all_labels) == i) > 0])\n",
    "    test_recall = np.mean([np.mean(np.array(all_preds)[np.array(all_labels) == i] == i) for i in range(3) if np.sum(np.array(all_labels) == i) > 0])\n",
    "    confuse_matrix = np.zeros([3,3])\n",
    "    for i in range(len(all_labels)):\n",
    "        confuse_matrix[all_preds[i]][all_labels[i]] += 1 \n",
    "    for i in range(3):\n",
    "        p = precision(confuse_matrix, i)\n",
    "        r = recall(confuse_matrix, i)\n",
    "        b = 1\n",
    "        print(f\"{i} -> \\nprecision ({p})\")\n",
    "        print(f'recall ({r})')\n",
    "        print(f'f-measure ({fmeasure(p, r, b)})')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
